\documentclass[10pt,onecolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Homework 4: Spring 2018. EN. 600.683: Vision as Bayesian Inference.\\  Due 17/May. 2018.}

\author{Richard Chen}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both


\maketitle
\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
The homework assigning is based on the course notes for lectures 20-25.
These  questions should have short answers. A few sentences, some mathematics, but not long mathematical derivations.
\end{abstract}


%%%%%%%%% BODY TEXT
\section*{Support Vector Machines 30 points}

\begin{enumerate}

\item  \textbf{What is the margin of an SVM? How does SVM deal with non-separable data? What is the primal formulation of SVM? What is the hinge loss? How does the SVM objective function relate to the empirical risk? What term helps prevent over-fitting to the training data? How does learning an SVM differ from learning Gaussian distributions for the positive and negative data examples and then applying the log-likelihood rule?}

Given a training example $(x^{(i)}, y^{(i)})$, the \textbf{functional margin} of an SVM w.r.t. to training sample is $\widehat{\gamma}^{(i)} = y^{(i)})(w^Tx+b)$, which represents confidence in making a correct prediction (for a binary classification task, if $y^{(i)}) = 1$, for the functional margin to be large, we need $w^Tx+b)$ to be a large positive number). The geometric margin is a scale invariant (canonical) form of the functional margin: $\widehat{\gamma}^{(i)} = y^{(i)})(w^Tx+b)$. Given a training set $ S = \{(x^{(i)}, y^{(i)}); i=1,\dots,m\}$, we also the define the \textbf{geometric margin} of $(w,b)$ w.r.t. to $S$ to be the smallest of the geometric margins of the individual training examples: $\gamma = \text{min}_{i=1,\dots,m} \gamma^{(i)}$. This margin is the gap, in which we want to maximize the geometric margin of the hyerplane that best separates the positive and negative training examples with this gap. 

In its canonical form, the \textbf{primal optimization problem} is defined as: $\text{min}_w f(w)$ s.t. $g_i(w) \leq 0, i=1,\dots,k; h_i(w)=0, i=1,\dots,l$, which we can solve by defining the generalized Lagrangian: $\mathcal{L}(w,\alpha, \beta) = f(w) + \sum_{i=1}^k \alpha_ig_i(w)+\sum_{i=1}^l\beta_ih_i(w)$, where $\alpha_i,\beta_i$ are the Lagrange multipliers. In the form of the Lagrangian, we can consider the primal optimization problem to be: $\text{min}_w\theta_{\mathcal{P}}(w) = \text{min}_w \text{max}_{\alpha,\beta:\alpha_i \geq 0} \mathcal{L}(w,\alpha,\beta)$. For the previous optmization problem in finding the optimal classifier: $\text{min}_{\gamma,w,b} \frac{1}{2}||w||^2$ s.t. $y^{(i)}(w^tx^{(i)}+b \geq 1, i=1,\dots,m$, we can construct to Lagrangian for our optimization problem to be: $\mathcal{L}(w,b,\alpha) -\frac{1}{2}||w||^2 - \sum_{i=1}^m \alpha_i[y^{(i)}(w^tx^{(i)}+b)-1]$.

SVM deals with \textbf{non-separable data} by reformulating the optimization with $l_1$ regularization as follows: $\text{min}_{\gamma,w,b} \frac{1}{2}||w||^2 + C \sum^m_{i=1}\xi_i$ s.t. $y^{(i)}(w^tx^{(i)}+b \geq 1 - \xi_i, i=1,\dots,m; \xi_i \geq 0, i=1,\dots,m$. This regularization allows exampels to have a functional margin less than 1, in which for examples with functional margin $1-\xi_i, \xi_i > 0$, it would pay the cost of the objective function being increased by $C\xi_i$.

The hinge loss is used for maximum-margin classification, which is mathematically defined as: $\trinagle(y_i, \widehat{y}_i(w)) = \text{max}(0,1-y_iw\cdot\phi(x_i))$, in which for $y_iw\cdot\phi(x_i) > 0, |y| \geq 1$ (same signs), the hinge loss is zero, but when they have opposite signs, the hinge loss increases linearly with $y$.
    
    
\item  \textbf{What is a kernel? How does it relate to feature vectors? What type of kernel makes an SVM behave like a nearest neighbor classifier?}

A \textbf{kernel} is a function used to transform an existing feature space to another feature space. Mathmetically, suppose we have a mapping: $\phi: \mathcal{R}^n \rightarrow \mathcal{R}^m$, which brings our feature vector in $\mathcal{R}^n$ to some feature space in $\mathcal{R}^m$. The dot product of two feature vectors $x,x'$ in this space is $\phi(x)^T\phi(x)$, and a kernel function $K$ is a function that corresponds to this dot product: $K(x,x') = \phi(x)^T\phi(x')$. It relates to \textbf{feature vectors} in which we can transform an existing feature space to occupy a higher dimensionality, s.t. it is easier to linearly separate data via higher-planes. A very studied example is the XOR problem, in which in two dimensions, it is impossible to find a linear classifier. However, we can define a feature transformation $\phi(x_1, x_2) = (x_1, x_2, x_1x_2)$, in which the classifier sign $\text{sign}\{(0,0,1)\cdot\phi(x_1,x_2) \} can separate the data. In addition, we do not need to specify the features $\phi(x)$ explicitly, as we only need to specify the kernel $K(x,x') = \phi(x)\cdot\phi(x')$. The Gaussian kernel, $e^{-\frac{1}{\sigma^2}|x-x'|^2}$, is the similar to \textbf{nearest neighbors.}

\item \textbf{The primal formulation is given by $ L_p(\vec a, b, \{z_i\}; \{\alpha_i, \mu_i\}) = (1/2) |\vec a|^2 + \gamma \sum _{i=1}^m z_i - \sum _{i=1}^m \alpha _i \{ y_i (\vec a \cdot \vec x_i + b) - (1 -z_i)\} - \sum _{i=1}^m \mu _i z_i.$ Explain the meaning of all the terms and variables in this equation. What constraints do the variables satisfy?  Calculate the form of the solution $\vec a$ by minimizing $L_p$ with respect to $\vec a$. What are the support vectors? How can the dual formulation be obtained by eliminating $\vec a, b, \{z_i\}$ from $L_p$. How can the primal problem be solved by exploiting the dual formulation?}

In the above primal formulation fr SVM, $\vec a, b$ is the plane, $C$ is the margin, $z_{i}$ are the slack variables, $\alpha_i, \mu_i$ are the Lagrange parameters, and $\gamma$ are the weights. The criterion is used the maximize the margins and minimize the amount of slack variables used, and the slack variables allow data points to move in direction $\vec a$. In particular, we seek a classifier with the biggest margin: $\text{max}_{\vec a, b, |\vec a|=1} C \text{ s.t. } y_{i}(\vec x_{i}\cdot\vec a + b) \geq C, \forall i \geq 1\dots N $, i.e., the positive examples are at least distance $C$ above the plane, and negative exampels are at least $C$ below the plane. The constraints satisfied are that $\alpha_i \geq 0, \mu_i \geq 0, \forall i$. To solve the above Quadratic Primal Problem, we need to minimize $\mathcal{L}_p(\vec a, b, z; \alpha, \mu)$ w.r.t. the primal variables $\vec a, b, z$ and maximized w.r.t. the dual variables $\alpha, \mu$. We can take the derivatives of the primal w.r.t. to its variables:

$$ \frac{\partial \mathcal{L}_p}{\partial \vec a} = 0 \rightarrow \widehat{\vec{a}} = \sum_i \alpha_iy_i\vec{x}_i $$
$$ \frac{\partial \mathcal{L}_p}{\partial b} = 0 \rightarrow \sum_i \alpha_iy_i = 0 $$
$$ \frac{\partial \mathcal{L}_p}{\partial z} = 0 \rightarrow \alpha_i = \gamma - \widehat{mu}_i, \forall i $$

The support vectors are $x_i$s with non-zero $\alpha_s$ which help define the margins of the hyperplane. To obtain the dual formulation of the primal, we can write the primal as $\mathcal{L}_p = -\frac{1}{2}\vec a \cdot \vec + \sum_i \alpha_i + \vec a \cdot (\vec a - \sum_i \alpha_iy_i\vec{x}_i) + \sum_i z_i(\gamma - \mu_i - \alpha_i) - b\sum_i\alpha_iy_i$. Note that: $\widehat{\vec{a}} = \sum_i \alpha_iy_i\vec{x}_i, \sum_i \alpha_iy_i=0, \gamma-\mu_i-\alpha_i=0$. Thus, $\mathcal{L}_p = -\frac{1}{2}\sum_{i,j}\alpha_i\alpha_jy_iy_j\vec{x}_i\vec{x}_j+\sum_i\alpha_i$. Now, with the dual formulation above, we have a maximization problem in which the parameters are in $\alpha_i$s.


\end{enumerate}

\section*{Structured Latent Support Vector Machine 30 points}


\begin{enumerate}

\item  \textbf{Describe how SVM's can be extended to learning the parameters for a multi-class classification problem where the output is of form $\vec y ^{\ast} = \arg \max _{\vec y} \lambda \cdot \phi (\vec d, \vec y)$, where $\vec d$ is the input and $\vec y$ is the output.}

We can extend SVMs to learn parameters for a \textbf{multi-class classification problem} with the following primal optimization:

$$ \mathcal{L}_p(w,z,\alpha) = \frac{1}{2}|w|^2 + c\sum_iz_i-\sum_i\sum_y\alpha_y^i(z_i-l(y,y_i)-w\phi(x_i,y)+w\phi(x_i,y_i))$$

in which the constraints are: $z_i-l(y,y_i)-w\phi(x,y)+w(x_i,y_i)\geq 0, \text{max}_y\{l(y,y_i)+w\phi(x_i,y)-w\phi(x_i,y_i)\} \leq z_i$. We can solve the primal formulation by:

$$ \dfrac{\partial \mathcal{L}_p(w,z,\alpha)}{\partial w} = 0 \rightarrow w = \sum_i\sum_j\alpha_y^i\{\phi(x_i,y_i)-\phi(x_i,y)\}$$

$$ \dfrac{\partial \mathcal{L}_p(w,z,\alpha)}{\partial z_i} = 0 \rightarrow c = \sum_y\alpha_y^i$$

This gives soluations $w(\alpha),z(\alpha)$, and thus, we can formualte the dual formulation:

$$ \mathcal{L}_d(\alpha) = \mathcal{L}_p(w(\alpha),z(\alpha),\alpha) = \sum_i\sum_y\alpha_y^il(y,y_i) - \frac{1}{2}\sum_{i,j}\sum y,z\alpha_y^i\alpha_z^i $$


\item \textbf{Now consider an SVM learnt over variable defined on a graph structure (e.g., like an HMM). How does the potential terms $\phi (\vec d, \vec y)$ determine the connections/edges in the graph structure? How does the form of $\lambda \cdot \phi (\vec d, \vec y)$ affect the difficulty of the inference and learning.}

For an \texbf{SVM learnt over variables defined in a graph structure} $G=(V,E)$, consider a potential $\phi(x,y)$ constructed from an MRF: $f(x,y) = \sum_{k\in\text{cliques}(G)}\phi_k(y_{\{k\}})$. For each input vector $x \in \mathcal{R}^m$, we can construct an MRF with a vertex for each $l$ possible labels $u$ associated with a weight vector $w_u \in \mathcal{R}^m$, and the resulting vertex potentials $\phi_u(1) = w_u^tx$, and the edge potentials $\phi_{uv}(1,1)$ come from individual values in $w$, one for each label pair. Back to the MRF network potential construction, the loss function is restricted to linearly decomposiable cliques, i.e., $\delta(y,\widehat{y}) = \sum_{k\in\text{cliques}(G)} \delta_k(y_{\{k\}},\widehat{y}_{\{k\}})$, $y$ is the value assignment to variables, $\delta_k$ are sub-component local loss functions, $\phi_k$ are potential functions representing the fitness of variable assignment $y_{\{k\}}$ to clique $k$, and $f(x,y)$ is the network potential that serves as a discriminant function representing the variable assignment $y$ in the structural SVM (Taskar et al. 2003).

This is \textbf{made difficult} because in the Structure SVM general formulation: $\mathcal{R}(\lambda) = \frac{1}{2}||\lambda||^2 + C\sum_{i=1}^M\triangle(y_i,\widehat{y}_i(\lambda)), \widehat{y}_i(\lambda) = \text{max}_y \lambda\phi(d,y_i$, it is both an intractable inference and non-convex problem. Specifically, it requires an inference algorithm for binary classification inference, because the error term $\triangle(y_i,\widehat{y}_i(\lambda))$ makes solving $R(\lambda)$ highly non-convex, it requires a tight, convex bount over $\R(\lambda)$ s.t. $y_i \approxto \widehat{y}_i(\lambda)$.

\item \textbf{Describe how to learn latent SVMs where some output variables $\vec y$ are observed, some variables $\vec h$ are hidden/latent, and the input data is $\vec d$. How can this be expressed in terms of minimizing an energy function which is composed of a convex and a concave term? How can CCCP be used to perform the learning?}

Consider the decision rule: $(\widehat{y},\widehat{h}) = \text{max}_{(y,h)\in\mathcal{Y},\mathcal{H}} \lambda \cdot \phi(d,y,h)$, where $h$ are the hidden variables, $d$ is the input variables, and $y$ are observed, with optimization: $\mathcal{R}(\lambda) = \frac{1}{2}||\lambda||^2 + C\sum_{i=1}^M\triangle(y_i;\widehat{y}_i(\lambda), \widehat{h}_i(\lambda)) \rightarrow $\mathcal{R}(\lambda) = \frac{1}{2}||\lambda||^2 + C\sum_{i=1}^M\text{max}_{(\widehat{y},\widehat{h})}(\triangle(y_i;\widehat{y}_i, \widehat{h}_i) + \lambda \cdot \phi(x_i,\widehat{y}, \widehat{h})) - \text{max}_h \lambda \cdot \phi(x_i,y_i,h) \rightarrow \frac{1}{2}||\lambda||^2 + C\sum_{i=1}^M f(\lambda) - g(\lambda)$, where $f(\lambda) = \text{max}_{(\widehat{y},\widehat{h})}(\triangle(y_i;\widehat{y}_i, \widehat{h}_i), g(\lambda) = \text{max}_h \lambda \cdot \phi(x_i,y_i,h)$, and are convex and concave respectively. Because \textbf{we can represent the optimization as a composition of a convex and concave function}, which we can solve this optimization using the cave-convex procedure (CCCP). CCCP is used to solve this optimization problem with the following two steps: 1) finding a hyperplane $q_t$ s.t. $-g(\lambda) \leq -g(\lambda_t) + (\lambda-\lambda_t)\cdot q_t, \forall \lambda$, and 2): Solve $\lambda_{t+1}=\text{min}_\lambda[f(\lambda) + \lambda \cdot q_{t}]$. In particular, we want to estimate the latent variables $h^{t} = \text{max}_h \lambda^t\phi(x_i,y_i,h)$ by the best estimates given by the current values of $\lambda^t$ (similar to to EM).

$$ \text{min}_\lambda \frac{1}{2}||\lambda||^2 + C\sum_{i=1}^M f(\lambda) - C\sum_{i=1}^M g(\lambda) $$

which is the standard structural SVM problem, which we canconvert to the dual formulation:

$$ \text{max}_{\alpha}\sum_{i,y,h}\alpha_{i,y,h}L_{i,y,h}-
frac{1}{2}C\sum_{i,j}\sum_{y,h,y',h'} \alpha_{i,y,h}\alpha_{j,y',h'}\triangle\Phi_{i,y,h} \cdot \triangle\Phi_{j,y',h'} $$

which we can solve using the cutting plane method, and 2), apply structural SVM learning to estimate the parameters $\lambda^{t+1}$ using the current estimates of the latent variables $h^{t}$, which has inference task

\end{enumerate}

\section*{Pictorial Structure Models and Deformable Parts Models: 30 points}


\begin{enumerate}

\item What is a pictorial structure model? How can this be expressed as probabilistic distribution, or energy function, defined over a graphical model? What are the state variables of the graph? What are the unary and binary terms? What inference algorithms can be used and how do they depend on the graph structure?

From Felzenswalb et al., a \textbf{pictorial structure model} for an object is given by a collection of parts with connections between certain pairs of parts, independent of the specific scheme used to model the appearance of each part as well as the type of connections between parts. We can express this as an undirected graph $G = (V,E)$, where the vertices $V = \{v_1,\dots,v_n\}$ correspond to the $n$ parts, there is an edge $(v_i,v_j)\in E$ for each pair of connected parts $v_i$ and $v_j$, and the instance of the object is given by a configuration $L=(l_1,\dots,l_n)$, where each $l_i$ specifies the location of part $v_i$. We can write the energy function/optimization as:

$$ L^{*} = \text{min}_L(\sum_{i=1}^nm_i(l_i) + \sum_{(v_i,v_j)\in E}d_{ij}(l_i,l_j)) $$

which is a configuration minimizing the sum of the match costs $m_i$ (unary potential) for each part and the deformation costs $d_ij$ (pairwise potential) for the connected pairs of parts. From Xu \& Yuille 2014, we can model this as as a probability distribution/graphical model, in which for a graph $G=(V,E)$ where the nodes in $V$ are the position of parts and the edges $E$ indicate which parts are spatially related, we can define the position of the parts as $l_i$ (where $l_i = (x,y)$ specify a pixel location of part $i$ for $i \in \{1,\dots,K\}$), and for each edge $(i,j) \in E$, we specify a discrete set of spatial relationships indexed by $t_{ij}$, which corresponds to a mixture of different spatial relationships (set of spatial relationships $t = \{t_{ij}, t_{ji}|(i,j)\in E}$). For an image $I$, we can define a score function $F(l,t|I)$ as an inference problem that is a sum of unary and pairwise terms, in which the unary term gives local evidence for part $i \in V$ to lie at location $l_i$, of form: $U(l_i|I) = w_i\phi(i|I(l_i);\theta)$, and the pairwise term captures intuition that neighboring parts $(i,j)\in E$ can roughly predict their relative spaatial positions using only local information. The full score $F(l, t|I)$ is thus defined by:

$$ F(l,t|I) = \sum_{i\in V}I(l_i|I) + \sum_{(i,j)\in E} R(l_i,l_j,t_{ij},t_{ji}|I)+w_o$$

To detect the optimal configuration for human pose in a pictural structure model, we search for configurations of locations $l$ and types $t$ that maximize score function: $(l^{*}, t^{*}) = \text{max}_{l,t}F(l,t|I), which requires the graph to be an acyclic graph/tree, which can be done efficiently using backward pass of dynamic programming/Viterbi and belief propagation algorithms. In addition, it also requires a restriction imposed on the form of connections between parts, as we need to specifiy a Mahalanobois distance between translocations. This restriction ensures that the running time  linear rather than quadratic in the number of possible locations for each
part.

\item How can the unary terms be modeled by deep networks? What are the advantages of using deep networks compared to alternative approaches?

As described from above, the unary term gives local evidence for part $i \in V$ to lie at location $l_i$, of form: $U(l_i|I) = w_i\phi(i|I(l_i);\theta)$, and does not try to capture spatial context between parts. As a result, because the unary term only give proposals on parts from an image patch, we can replace the unary term with a Deep Convolutional Neural Network (DCNN) to learn the probability distribution $p(c,m_{c\mathcal{N}(c)}|I(l_i);\theta)$. For a set of labelled patches $\{I(l^n),c^n,m^n_{c^n\mathcal{N}(c^n)}\}^{KN}_{n=1}$ from positive images (each positive image provides $K$ part patches) and also a set of background patches sampled from negative images, instead of capturing features through HOG filters, we can train a multi-class DCNN clasifier to learn parts with category label $c^n \in \{1,\dots,K\}$ and the background. DCNNs are unable to fail to capture the full context of all body parts at the image-level, however, at the patch-level, they can learn powerful local features without having to rely on HOG filters, which can have sparse/flat representations of only short-range spatial interactions between parts of hte object.

\item \textbf{What are deformable part models? What types of cues are used for the unary terms?  Do you have different models for different viewpoints? What is the form of the spatial interactions? How can structure latent SVM be used to learn the parameters of deformable parts models? What do the latent variables correspond to?}

Deformable Parts Models model an object class as a mixture of components, each being responsible for modeling the appearance of an object sub-category. Each component uses a low-resolution global appearance model ("root filters") of the sub-type, with higher resolution "part filters" that capture the appearance of local regions of the sub-type. The intuition is that the local appearance is easier to model than the global appearance, as training data is shared across deformations, "parts” can be local or global depending on resolution, which can help the model generalize to unseen configurations. Previously, computer visions hhave been able to reliable parse deformable objects, as previous methods relied on a sparse set of image cues such as SIFT features or edgelets, and flat representations of the spatial interactions between the different parts of the object. Many attempts to model deformable bojects in the past have formulated the problem as a maximum a posteriori inference of the pose and states $z$ of the object parts in terms of input image data $I$. Formally, we seek to estimate: $z^{*} = \text{max}_z p(z|I) = \text{max}_zp(I|z)p(z)$, where $p(I|z)p(z) = p(I,z)$ is of the form:

$$ p(I,z) = \frac{1}{Z}\text{exp}\{\sum_i\alpha_if(I(x_i),z_i) + \sum_{i,j}\beta_{ij}g(z_i,z_j)\} $$, where $Z$ is the normalization constant, $x_i$ is the image position, and the unary potentials $f(I(x_i), z_i)$ model how well the individual features match to the position in the image, and the pairwise potentials $g(z_i,z_j)$ impose probabilistic constraints on the spatial relationships between feature points. Traditionally, you would have Deformable Parts Models for different stages of resolutions, as certain models would be only good for catching certain cues (SIFT features, Wavelets), i.e., models that used longer spatial interactions encoded more shape context and global evaluations. Another example, sonsider the problem of modeling the appearance of bicycles in photographs. People build bicycles of different types (e.g., mountain bikes, tandems, and 19th-century cycles with one big wheel and a small one) and view
them in various poses (e.g., frontal versus side views). The system described here uses mixture models to deal with these more significant variations. In the work by Zhu \& Yuille 2010, however, they propose a class of object model—hierarchical deformable templates, which specify a hierarchical graph, where nodes at different levels of the hierarchy represent components of the object at different scales, which attempts to solve this task. Felzenswalb et al. 2010 was able to show that a Stuctural Latent SVM was able to learn parameters for a deforamble parts by allowing it to learn rich, visual grammars. Consider a classifier that scores an example $x$ with a decision rule: $f_\beta(x) = \text{max}_{z \in Z(x)} \beta \cdot \phi (x,z)$, where $\beta$ is a vector of model parameters and $z$ are latent vectors. In the case of "star models" that are commonly used in deformable parts models, $\beta$ is the concatenation of the root filter, the part filters, and deformation cost weights, and the latent vector $z$ is a specification of the object configuration, and $\phi(x,z)$ is a concatenation of subwindows from a feature pyramid and part deformation features. By solving the concave-convex prolem of latent structured SVMs (using CCCPs), we can learn $\beta$s that define the features of the deformation parts, and $\zeta$s that define visual grammars.



\end{enumerate}

\section*{Deep Networks Attacks and Understanding: 30 points}


\begin{enumerate}

\item \textbf{What are adversarial examples for deep networks? How can the backpropagation algorithm be modified in order to generate them? What happens if you use the same algorithm and start with random noise? What if, in addition, you enforce natural image statistics by using regularizers?}
Adversial examples are imperceptible perturbations that when applied to an input of a neural network, causes the neural network to misclassify. A linear view of adversarial example generation was proposed by Goodfellow et al. 2015., in which for model parameters $\theta$, model input $x$, targets $y$ associated with $x$, and loss function $J(\theta, x, y)$ used to train the network, we can linearize the cost function around the current value of $\theta$, obtaining an optimal max-norm constrained perturbation of $\nu = \epsilon\text{sign}(\triangledown_xJ(\theta,x,y))$. This is the "fast gradient sign method" of generating adversarial examples, can can be computed efficiently with backpropagation. Using the same algorithm and starting with random noise will have little affect in preventing adversarial examples, in which the expected dot product etween any reference vector and such a noise vector is zero, which results in that in many cases, the noise wil lhave essentially no effect rather than yielding a more difficult input. In same cases, it has been shown that adding noise will actually result in a lower objective function. Enforcing natural image statistics as a regularizer w

\item \textbf{What is a strategy to estimate what different neurons in the deep network correspond to? Does it make a difference if the deep network is trained to classify scenes or objects? How can we classify activity patterns of groups/populations of neurons to discover visual concepts?}

To estimate what different neurons in the deep networks correspond to, we can visualize different layers of the neural network, and build visual concepts of what the neural network is learning and activating on at its receptive fields. Depending on the supervised task at hand, \textbf{It does not} make a difference if the deep network is trained to classify scenes or objects, as with enough channels, the network would build feature representations

We can obtain visual concepts by doing K-means clustering and merging feature vectors into a set $\mathcal{V}$ of visual concepts, and/or modifying deep networks to includ additional \textit{vc} neurons which are connected to neurons by softmax layers, which allows us to find visual concepts naturally. This can be formalized as unsupervised learning where the data, i.e., the feature vectors $\{f_v\}$ are generated a by a mixture of von Mises-Fisher distributions, which can further formulated in terms of neural networks where the cluster centers (visual concepts), are specified by the visual concept neurons with weights $\{f_v\}$.

\end{enumerate}

\section*{THE FOLLOWING TWO QUESTIONS ARE OPTIONAL AND OPEN ENDED. THEY WILL NOT AFFECT THE GRADING}

\section*{Compositional Models and AND-OR graphs: Optional Points}

\begin{enumerate}

\item How do compositional models represent objects in terms of elementary parts? What is part sharing? What are its advantages? What are the challenges of using compositional models?

\item What are AND-OR graphs? How do they differ from mixture models of objects?

\item What is structure induction? Briefly describe one strategy for structure induction.

\end{enumerate}

\section*{Region Competition, and Image Parsing: Optimal Points}

\begin{enumerate}


\item What is region competition? How does this differ from classic image segmentation models like the weak membrane model?

\item What is image parsing? What is Metropolis-Hasting algorithm? What is data driven markov chain Monte Carlo? How can discriminative models be used to make proposals for generative models?

\end{enumerate}




\end{document}